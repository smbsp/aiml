{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction to Machine Learning & Neural Networks (With PlayGrounds) üöÄ\n",
        "\n",
        "**Welcome!** 2-hour exploration into the fascinating realms of Machine Learning (ML) and Neural Networks (NN).\n",
        "\n",
        "\n",
        "**Agenda:**\n",
        "* Grasp the fundamental definition and core ideas of Machine Learning.\n",
        "* Distinguish between the primary categories of Machine Learning algorithms.\n",
        "* Identify practical applications and use-cases for these algorithms.\n",
        "* Develop a foundational understanding of Neural Networks.\n",
        "* Interact with online tools to visualize ML concepts in action.\n",
        "* Learn the basics of how text is transformed into numerical vectors for machine processing.\n",
        "\n",
        "**Let's begin!**\n"
      ],
      "metadata": {
        "id": "uaE74CPN5Z-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why Machine Learning and not Software Engineering?\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1W3a-WULhWso2x0PbSR2Z_lxIJ5DaAaGk' width=800>\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1c2zHetGUEn6FNt1hEIajh-4iDSsoJ974' width=800>\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1PkREmb49ABwkpn5Pvf5Wlqn19MxRMg0S' width=800>\n"
      ],
      "metadata": {
        "id": "0RGwDiFQfYhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ñ Part 1: What is Machine Learning?\n",
        "\n",
        "\n",
        "**Machine Learning (ML)** is a subfield of Artificial Intelligence (AI) that focuses on building systems capable of learning from data and improving their performance on a specific task over time, without being explicitly programmed for each nuanced scenario. In simpler terms, instead of writing exact, step-by-step instructions for a task, we \"train\" a model by showing it numerous examples, allowing it to identify patterns and make decisions or predictions on its own.\n",
        "\n",
        "\n",
        "  <img src=\"https://cdn.prod.website-files.com/6064b31ff49a2d31e0493af1/66d00e5ec7365a6dea70c127_AD_4nXfVtjmcixZXnAhTKlN3oS0UeO5XAPxhBOdeWsglXLSlYwMTliz6nTY3-UGo4AX5VRS6Ex5AAaKrK5gI1VT-J_FNgYCxnTAT8ZxvftfVM24fTFmHGXS38dy53fSimQ3Uql5Dwmkd_0D3SAvTbst8PJd3Zca7.png\" alt=\"Linear Regression Example\" width=\"800\"/>\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1o13qceJo1AAEYr64TvMVZEvxZ0pybKpK' width=800>\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=18b8T00_p29IL-pswfhhcvmKsUPbq4KlE' width=800>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1EIscKR4MhkO3kRpDH86heigDiTXQUwjN' width=800>\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=19PyZnOFOBRHDVwpb1PKtFiB4L7pI7g-i' width=800>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1CnyLYmHfIsZhRsEJ5YvhvZCD1lGJckZg' width=800>\n",
        "\n",
        "\n",
        "**Core Concepts:**\n",
        "* **Data:** The lifeblood of ML. Algorithms learn patterns and relationships from data. The quality, quantity, and relevance of data are paramount.\n",
        "* **Model:** A mathematical or computational representation learned from the data. This is the \"brain\" that makes predictions or decisions.\n",
        "* **Training:** The process of feeding data to an ML algorithm to enable the model to learn the underlying patterns or mapping function.\n",
        "* **Inference/Prediction:** The stage where the trained model is used to make predictions or decisions on new, previously unseen data.\n",
        "* **Features:** These are the input variables or attributes used by the model to make predictions. For instance, in predicting a house's price, features could include its size (sq. ft.), number of bedrooms, and location.\n",
        "* **Labels/Targets:** In supervised learning, this is the output variable we aim to predict (e.g., the actual price of a house in the training dataset).\n",
        "\n",
        "**Why is ML Important in Today's World?**\n",
        "* **Solving Complex Problems:** ML excels at tackling problems that are too intricate for humans to define with explicit rules, such as recognizing faces in images or understanding spoken language.\n",
        "* **Automation of Tasks:** It can automate repetitive or mundane tasks, freeing up human resources for more creative or strategic endeavors.\n",
        "* **Personalization at Scale:** ML powers recommendation engines (like those on Netflix or Amazon) and enables personalized user experiences across various applications.\n",
        "* **Deriving Insights from Big Data:** It helps uncover hidden patterns, trends, and valuable insights from vast and complex datasets.\n"
      ],
      "metadata": {
        "id": "QH4saODS5ifB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è Part 2: Types of Machine Learning Algorithms\n",
        "\n",
        "\n",
        "\n",
        "Machine Learning algorithms can be broadly categorized into three main types based on the nature of the learning signal or feedback available to the learning system:\n",
        "\n",
        "1.  **Supervised Learning**\n",
        "2.  **Unsupervised Learning**\n",
        "3.  **Reinforcement Learning**\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=15ucp5k3RNxYtLtmSjfTmIiagIgtJR0_0' width=800>\n",
        "\n",
        "Let's delve into each category.\n",
        "\n",
        "## 1. Supervised Learning\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1dEFmIXNUzuhVEdJo3IwdYHT2kuRoY4Ct' width=800>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In supervised learning, the algorithm learns from **labeled data**. This means each data point in the training set is accompanied by a correct output or \"label.\" The goal is to learn a mapping function that can take new input data (without labels) and predict the corresponding output label.\n",
        "\n",
        "Think of it as learning with a knowledgeable teacher who provides examples (data) along with the correct answers (labels).\n",
        "\n",
        "**Common Tasks & Algorithms:**\n",
        "\n",
        "\n",
        "\n",
        " ### Regression: Predicting a continuous numerical value.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/600px-Linear_regression.svg.png\" alt=\"Linear Regression Example\" width=\"400\"/>\n",
        "  \n",
        "**Use-Cases:**\n",
        "\n",
        "        * Predicting house prices based on features like area, number of rooms, and age.\n",
        "        * Forecasting stock market prices or sales figures.\n",
        "        * Estimating the temperature for a future date.\n",
        "        * Predicting a student's score based on study hours.\n",
        "\n",
        "\n",
        "  **Popular Algorithms:**\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=130O-BY41EEukdtctjWK2nAnoBUgwyKID' width=800>\n",
        "\n",
        "  **1. Linear Regression:** Fits a linear equation to the data. Assumes a straight-line relationship between inputs and the output.\n",
        "\n",
        "\n",
        "  **2. Polynomial Regression:** Allows for curved relationships by fitting a polynomial equation.\n",
        "\n",
        "  **3. Support Vector Regression (SVR):** An adaptation of Support Vector Machines for regression problems.\n",
        "\n",
        "  **4. Decision Tree Regression & Random Forest Regression:** Tree-based methods that can capture complex non-linear relationships by partitioning the feature space.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### **Classification**: Predicting a discrete categorical label (i.e., assigning an input to one of several predefined classes).\n",
        "  ![1_aE8XLyApqvaQA9B7MWjjlA.webp](data:image/webp;base64,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)\n",
        "  \n",
        "    * **Use-Cases:**\n",
        "        * Email spam detection (classifying emails as \"spam\" or \"not spam\").\n",
        "        * Image classification (e.g., identifying images as containing a \"cat,\" \"dog,\" or \"car\").\n",
        "        * Medical diagnosis (e.g., determining if a tumor is \"benign\" or \"malignant\").\n",
        "        * Customer churn prediction (predicting whether a customer will \"churn\" or \"stay\").\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=180H1_PNOAk99IWOB5_p9iwt3Z7wnQ5C4' width=800>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1bNuJbKIQK5aCS2kBnpAj4Jjlnl3JGLez' width=800>\n",
        "\n",
        "**Popular Algorithms:**\n",
        "\n",
        "**1. Logistic Regression:** Despite \"regression\" in its name, it's used for classification. It predicts the probability of an instance belonging to a class.\n",
        "\n",
        "**2. K-Nearest Neighbors (KNN):** Classifies a new data point based on the majority class of its 'k' closest neighbors in the feature space. Simple yet effective.\n",
        "\n",
        "**3. Support Vector Machines (SVM):** Finds an optimal hyperplane that best separates data points of different classes in a high-dimensional space.\n",
        "\n",
        "**4. Decision Trees:** Creates a tree-like model of decisions. Each internal node represents a test on an attribute, each branch represents an outcome, and each leaf node represents a class label. Easy to interpret.\n",
        "\n",
        "**5. Random Forest:** An ensemble learning method that constructs multiple decision trees during training and outputs the class that is the mode of the classes output by individual trees. Improves accuracy and controls overfitting.\n",
        "\n",
        "**6. Naive Bayes:** A probabilistic classifier based on Bayes' theorem, with a \"naive\" assumption of independence between features. Works well for text classification tasks like spam filtering.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MdfIgWnX5meg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1p9pPCDtOwlRTWrVGfaCMIOKXtyWd0xjC' width=800>\n"
      ],
      "metadata": {
        "id": "Lp_9MXbLdHV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1Q8MFunwq7Boq7Y_XxMXuBpAnmxvJb2S_' width=800>\n"
      ],
      "metadata": {
        "id": "7xUnXtJddM0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=15X2o5B4PPEILKxq7kPQL0srfsKEBGYuj' width=800>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1CnyLYmHfIsZhRsEJ5YvhvZCD1lGJckZg' width=800>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1GhkArAAxjd2pNfChOYWe8TAoRWhAw2ID' width=800>\n"
      ],
      "metadata": {
        "id": "Jx9X8ReSddk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Unsupervised Learning\n",
        "\n",
        "In unsupervised learning, the algorithm learns from **unlabeled data**. The goal is to discover hidden patterns, structures, or relationships within the data itself, without any predefined outputs or explicit guidance.\n",
        "\n",
        "Think of it as learning without a teacher, where you explore the data to find interesting structures or groupings on your own.\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1kSFaQfYOpumsJbwLevt6v7J8J8oeBT_d' width=800>\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1-0Eg_CSwSOnyH2NCADL4V0TgAo97_HUp' width=800>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1wTUfSSW3G7_YB4fmovnrhm3DyDqfyZs2' width=800>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Common Tasks & Algorithms:**\n",
        "\n",
        "\n",
        "## **Clustering:** Grouping similar data points together based on their characteristics.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/K-means_convergence.gif/300px-K-means_convergence.gif\" alt=\"K-Means Clustering Example\" width=\"300\"/>\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1q2ldtYcjujOGIG4IUklwfD2pOnJY0sGR' width=800>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1Oi2lKKaK0rTm5GYqth8IWvyOgMmHrlB2' width=800>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1a6Bc0DPf-BWXXbcEF-uok8uwkkyKgv8P' width=800>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    * **Use-Cases:**\n",
        "        * Customer segmentation (e.g., grouping customers with similar purchasing behaviors for targeted marketing).\n",
        "        * Anomaly detection (identifying unusual data points that don't fit well into any cluster, e.g., fraud detection).\n",
        "        * Image segmentation (grouping pixels in an image to identify objects).\n",
        "        * Organizing documents by topic.\n",
        "    * **Popular Algorithms:**\n",
        "        * **K-Means Clustering:** Partitions data into 'k' distinct, non-overlapping clusters. It iteratively assigns data points to clusters and updates cluster centroids. Requires 'k' to be specified.\n",
        "        * **Hierarchical Clustering:** Builds a tree-like hierarchy of clusters. Can be agglomerative (bottom-up) or divisive (top-down). Does not require specifying 'k' beforehand.\n",
        "        * **DBSCAN (Density-Based Spatial Clustering of Applications with Noise):** Groups together points that are closely packed, marking as outliers points that lie alone in low-density regions. Can find arbitrarily shaped clusters.\n",
        "\n",
        "* **Dimensionality Reduction:** Reducing the number of input features (variables) while preserving essential information or structure.\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*37a_i1t1tDxDYT3ZI6Yn8w.gif\" alt=\"PCA Dimensionality Reduction Example\" width=\"600\"/>\n",
        "\n",
        "    * **Use-Cases:**\n",
        "        * Data visualization (reducing high-dimensional data to 2D or 3D for plotting and easier understanding).\n",
        "        * Improving the efficiency and performance of other ML algorithms by reducing computational complexity.\n",
        "        * Noise reduction and removal of redundant features.\n",
        "    * **Popular Algorithms:**\n",
        "        * **Principal Component Analysis (PCA):** A linear technique that transforms data into a new set of uncorrelated variables called principal components, ordered by the amount of variance they explain.\n",
        "        * **t-distributed Stochastic Neighbor Embedding (t-SNE):** A non-linear technique particularly effective for visualizing high-dimensional datasets in low-dimensional space (e.g., 2D or 3D maps).\n",
        "        * **Autoencoders:** A type of neural network used for learning efficient data codings (compressions) in an unsupervised manner. The encoder part reduces dimensionality, and the decoder part reconstructs the original input.\n",
        "\n",
        "* **Association Rule Mining:** Discovering interesting relationships or \"rules\" between variables in large datasets.\n",
        "    *(Conceptual: Often represented by \"if-then\" rules, e.g., {Diapers} -> {Beer})*\n",
        "    * **Use-Cases:**\n",
        "        * Market basket analysis (e.g., finding that \"customers who buy diapers are also likely to buy beer\").\n",
        "        * Recommendation systems (suggesting products based on items frequently bought together).\n",
        "        * Web usage mining.\n",
        "    * **Popular Algorithms:**\n",
        "        * **Apriori:** Identifies frequent itemsets in a dataset and then generates association rules from these itemsets based on confidence and support.\n",
        "        * **Eclat (Equivalence Class Clustering and bottom-up Lattice Traversal):** Another algorithm for frequent itemset mining, often faster than Apriori.\n",
        "\n",
        "## 3. Reinforcement Learning\n",
        "\n",
        "Reinforcement Learning (RL) involves training an **agent** to make a sequence of decisions in an **environment** to maximize a cumulative **reward**. The agent learns through trial and error, receiving feedback in the form of rewards or punishments for its actions.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Reinforcement_learning_diagram.svg/600px-Reinforcement_learning_diagram.svg.png\" alt=\"Reinforcement Learning Agent-Environment Loop\" width=\"500\"/>\n",
        "\n",
        "\n",
        "Think of it like training a pet: good behavior is rewarded, leading the pet to repeat it, while undesirable behavior might be ignored or lead to a negative outcome.\n",
        "\n",
        "**Core Concepts:**\n",
        "* **Agent:** The learner or decision-maker.\n",
        "* **Environment:** The external world with which the agent interacts.\n",
        "* **State:** A representation of the current situation or configuration of the environment.\n",
        "* **Action:** A choice made by the agent from a set of possible moves.\n",
        "* **Reward:** A scalar feedback signal from the environment that indicates how good or bad the agent's action was in a particular state.\n",
        "* **Policy:** The strategy or mapping that the agent uses to select actions based on the current state. The goal of RL is to find an optimal policy.\n",
        "\n",
        "**Use-Cases:**\n",
        "* **Robotics:** Training robots to perform complex tasks like walking, grasping objects, or navigation.\n",
        "* **Game Playing:** Developing AI agents that can play games at or above human level (e.g., AlphaGo for Go, OpenAI Five for Dota 2).\n",
        "* **Autonomous Systems:** Decision-making in self-driving cars or autonomous drones.\n",
        "* **Resource Management:** Optimizing energy consumption, traffic light control, or financial portfolio management.\n",
        "* **Personalized Recommendation Systems:** Learning user preferences over time to provide dynamically adapting recommendations.\n",
        "\n",
        "**Popular Algorithms/Approaches:**\n",
        "* **Q-Learning:** A model-free RL algorithm that learns the value (Q-value) of taking an action in a particular state.\n",
        "* **SARSA (State-Action-Reward-State-Action):** An on-policy RL algorithm similar to Q-learning.\n",
        "* **Deep Q-Networks (DQN):** Combines Q-learning with deep neural networks to handle complex, high-dimensional state spaces (e.g., learning from raw pixel data in video games).\n",
        "* **Policy Gradient Methods (e.g., REINFORCE, A2C, A3C):** Directly learn the policy function that maps states to actions, often by using gradient ascent to maximize expected reward.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "bqoBjRHK5q8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: A Glimpse into Neural Networks\n",
        "\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/600px-Colored_neural_network.svg.png\" alt=\"Neural Network Diagram\" width=\"500\"/>\n",
        "\n",
        "\n",
        "**Neural Networks (NNs)**, and their more complex counterparts in **Deep Learning**, are a powerful class of ML algorithms inspired by the structure and function of the human brain. They are the driving force behind many of the most significant recent breakthroughs in AI.\n",
        "\n",
        "**What are Neural Networks?**\n",
        "* They are modeled (loosely) after the biological neurons and their connections in animal brains.\n",
        "* Composed of interconnected layers of processing units called \"neurons\" or \"nodes.\"\n",
        "* Each connection between neurons has an associated \"weight,\" which is adjusted during the learning process. These weights determine the strength of the signal passed between neurons.\n",
        "* Neurons receive inputs, perform a weighted sum, and then apply an \"activation function\" to this sum to produce an output, which is then passed to other neurons.\n",
        "\n",
        "**Basic Structure:**\n",
        "1.  **Input Layer:** Receives the raw input data (the features of your dataset).\n",
        "2.  **Hidden Layers:** One or more layers between the input and output layers. These layers perform complex computations and transformations on the data, allowing the network to learn hierarchical features. \"Deep Learning\" typically refers to NNs with multiple hidden layers.\n",
        "3.  **Output Layer:** Produces the final result of the network (e.g., a class prediction in classification, a continuous value in regression).\n",
        "\n",
        "**How Do Neural Networks Learn?**\n",
        "The learning process in (supervised) NNs usually involves these steps:\n",
        "1.  **Forward Propagation:** Input data is fed into the input layer and travels forward through the hidden layers to the output layer, generating a prediction.\n",
        "\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YHM9vCaEf8pmdBycuW4wrg.gif\" alt=\"Forward Propagation in Neural Network\" width=\"600\"/>\n",
        "\n",
        "2.  **Loss Function:** A function that measures the discrepancy (error) between the network's prediction and the actual target label from the training data.\n",
        "   <img src=\"https://miro.medium.com/v2/resize:fit:1200/format:webp/0*SAuz0umDwZhde4TH.jpeg\" alt=\"Loss\" width=\"600\"/>\n",
        "\n",
        "3.  **Backpropagation:** The error is propagated backward through the network, from the output layer to the input layer. This process calculates how much each weight in the network contributed to the overall error.\n",
        "    *(Conceptual: Error signals flow backward, adjusting weights.)*\n",
        " <img src=\"https://miro.medium.com/v2/resize:fit:906/format:webp/0*CDJ9dTRiajBsowmB.png\" alt=\"Loss\" width=\"600\"/>\n",
        "\n",
        "\n",
        "4.  **Optimization (e.g., Gradient Descent):** An optimization algorithm (like Gradient Descent or its variants like Adam) adjusts the weights of the connections in the network in a direction that minimizes the loss function. This iterative process of forward propagation, loss calculation, backpropagation, and weight update is how the network \"learns.\"\n",
        "\n",
        "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Gradient_descent.svg/400px-Gradient_descent.svg.png\" alt=\"Gradient Descent Optimization\" width=\"450\"/>\n",
        "\n",
        "\n",
        "\n",
        "**Why are Neural Networks so Powerful?**\n",
        "* **Automatic Feature Learning:** Deep NNs can automatically discover and learn relevant features from raw data (e.g., edges and textures from images, word patterns from text), reducing the need for manual, time-consuming feature engineering.\n",
        "* **Handling Complex, High-Dimensional Data:** They excel at tasks involving unstructured data like images, audio, and text.\n",
        "* **State-of-the-Art Performance:** NNs have achieved groundbreaking results and are the leading approach in many challenging AI domains, including:\n",
        "    * **Computer Vision:** Image classification, object detection, facial recognition, image generation.\n",
        "    * **Natural Language Processing (NLP):** Machine translation, sentiment analysis, text summarization, question answering, and large language models like GPT.\n",
        "    * **Speech Recognition:** Converting spoken language into text.\n",
        "\n",
        "**Brief Overview of Neural Network Types:**\n",
        "* **Feedforward Neural Networks (FNNs):** The simplest type, where information flows in only one direction, from input to output, without cycles.\n",
        "* **Convolutional Neural Networks (CNNs or ConvNets):** Highly effective for processing grid-like data, especially images. They use special layers (convolutional, pooling) to detect local patterns and hierarchies of features.\n",
        "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Typical_cnn.png/600px-Typical_cnn.png\" alt=\"Convolutional Neural Network (CNN) Architecture\" width=\"550\"/>\n",
        "\n",
        "* **Recurrent Neural Networks (RNNs):** Designed for sequential data, such as text, speech, or time series, where the order of information matters. They have \"memory\" in the form of recurrent connections that allow information from previous steps to influence the current step. LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units) are advanced types of RNNs that address some limitations of simple RNNs.\n",
        "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Recurrent_neural_network_unfold.svg/600px-Recurrent_neural_network_unfold.svg.png\" alt=\"Recurrent Neural Network (RNN) Unfolded\" width=\"550\"/>\n",
        "\n",
        "* **Transformers:** A more recent and revolutionary architecture, particularly dominant in NLP. They use a mechanism called \"self-attention\" to weigh the importance of different parts of the input sequence when processing information, allowing for better handling of long-range dependencies. Models like BERT and GPT are based on the Transformer architecture.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "sC3aeukS5upq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üî¨ Part 4: Practical Exercises & Demonstrations\n",
        "\n",
        "\n",
        "Theory is essential, but seeing ML in action provides invaluable intuition! Let's explore some interactive web-based tools.\n",
        "\n",
        "1.  **Google Teachable Machine:**\n",
        "    * **What it is:** A user-friendly web tool that allows you to train simple ML models for image, sound, or pose classification directly in your browser‚Äîno coding required! It's excellent for understanding the basic workflow of training and testing a model.\n",
        "    * **Link:** [Google Teachable Machine](https://teachablemachine.withgoogle.com/)\n",
        "    * **Hands-on Exercise :**\n",
        "        1.  Navigate to the Teachable Machine website.\n",
        "        2.  Select an \"Image Project.\"\n",
        "        3.  Define 2-3 distinct classes (e.g., \"My Face,\" \"A Book,\" \"A Water Bottle\").\n",
        "        4.  For each class, use your webcam to capture sample images (aim for at least 20-30 diverse images per class for better results). Try varying angles and backgrounds slightly.\n",
        "        5.  Click the \"Train Model\" button. Wait for the training process to complete (it's usually quick).\n",
        "        6.  Once trained, test your model using the \"Preview\" pane with your webcam. See how accurately it classifies the objects or your face.\n",
        "        7.  **Discussion Points:**\n",
        "            * How does the number of training samples per class affect the model's accuracy?\n",
        "            * What happens if you try to classify an object the model hasn't seen before?\n",
        "            * How sensitive is it to changes in lighting or background?\n",
        "            * This exercise demonstrates supervised learning (specifically, image classification).\n",
        "\n",
        "2.  **TensorFlow Playground:**\n",
        "    * **What it is:** An interactive in-browser visualization of a neural network. You can design a simple neural network, select datasets, adjust hyperparameters (like learning rate, activation functions, number of layers/neurons), and watch the network learn to classify data in real-time.\n",
        "    * **Link:** [TensorFlow Playground](https://playground.tensorflow.org/)\n",
        "\n",
        "        1.  Open TensorFlow Playground.\n",
        "        2.  Familiarize yourself with the interface:\n",
        "            * **Data:** On the left, choose a dataset (e.g., Circle, Exclusive OR (XOR), Spiral). These represent different classification challenges.\n",
        "            * **Features:** Select which input features to use.\n",
        "            * **Hidden Layers:** Add or remove hidden layers and change the number of neurons in each.\n",
        "            * **Output:** Observe the classification output visualized on the right, and track metrics like \"Loss.\"\n",
        "        3.  Click the \"Play\" button (‚ñ∂Ô∏è) to start the training process. Watch how the decision boundary (the colored regions) evolves and the training/test loss changes.\n",
        "        4.  **Experiment with:**\n",
        "            * Changing the **Learning Rate**: See how very small or very large rates affect training.\n",
        "            * Trying different **Activation Functions** (e.g., ReLU, Sigmoid, Tanh).\n",
        "            * Adding more layers or neurons: Does a more complex network always perform better?\n",
        "            * Switching datasets: Which datasets are harder for the network to learn?\n",
        "        5.  **Discussion Points:**\n",
        "            * What does the \"Loss\" value represent?\n",
        "            * Can you observe \"overfitting\" (good performance on training data, poor on test data)?\n",
        "            * How do different hyperparameters interact?\n"
      ],
      "metadata": {
        "id": "RWkq05Mx5yPZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìú Part 5: How Machines \"Understand\" Text: Converting Text to Vectors\n",
        "\n",
        "\n",
        "Machines, at their core, understand numbers, not words or sentences in the human sense. For ML algorithms to process and \"understand\" textual data, we must first convert it into a numerical format, typically vectors. This process is known as **Text Vectorization** or **Feature Extraction from Text**.\n",
        "\n",
        "Here are some common fundamental techniques:\n",
        "\n",
        "1.  **Bag-of-Words (BoW):**\n",
        "    * **Concept:** Represents a piece of text as an unordered collection (a \"bag\") of its words, disregarding grammar and word order but keeping track of word frequency.\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*axffCQ9ae0FHXxhuy66FbA.png\" alt=\"Bag of Words Concept\" width=\"800\"/>\n",
        "  \n",
        "    * **How it works:**\n",
        "        1.  **Tokenization:** Break the text into individual words (tokens).\n",
        "        2.  **Vocabulary Creation:** Compile a list of all unique words across the entire collection of documents (the corpus). This vocabulary determines the dimension of our vectors.\n",
        "        3.  **Vector Creation:** For each document, create a vector where each component corresponds to a unique word in the vocabulary.\n",
        "        4.  **Value Assignment:** The value for each component in a document's vector can be:\n",
        "            * **Binary:** 1 if the word from the vocabulary is present in the document, 0 otherwise.\n",
        "            * **Frequency Count:** The number of times the word appears in the document.\n",
        "    * **Example:**\n",
        "        * Document 1: \"The cat sat on the mat.\"\n",
        "        * Document 2: \"The dog ate the cat.\"\n",
        "        * Corpus Vocabulary: {\"The\", \"cat\", \"sat\", \"on\", \"mat\", \"dog\", \"ate\"} (typically lowercased, punctuation removed)\n",
        "        * BoW Vector for Doc 1 (frequency): `[2, 1, 1, 1, 1, 0, 0]` (if \"the\" is counted twice)\n",
        "        * BoW Vector for Doc 2 (frequency): `[2, 1, 0, 0, 0, 1, 1]`\n",
        "    * **Pros:** Simple to understand and implement. Often works well for basic text classification tasks.\n",
        "    * **Cons:**\n",
        "        * Loses information about word order and sentence structure (e.g., \"man bites dog\" and \"dog bites man\" might look similar).\n",
        "        * High dimensionality for large vocabularies (sparse vectors).\n",
        "        * Very common words (stopwords like \"the\", \"is\", \"a\") can dominate the vector without adding much meaning, unless filtered out.\n",
        "\n",
        "2.  **TF-IDF (Term Frequency-Inverse Document Frequency):**\n",
        "    * **Concept:** A more sophisticated numerical statistic that reflects how important a word is to a document in a collection or corpus. It gives higher weight to words that are frequent in a specific document but rare across all documents.\n",
        "\n",
        "    * **How it works:**\n",
        "        * **Term Frequency (TF):** Measures how frequently a term (word) $t$ appears in a document $d$.\n",
        "           `\n",
        "        * **Inverse Document Frequency (IDF):** Measures how much information a word provides, i.e., whether it's common or rare across all documents $D$ in the corpus.\n",
        "            \n",
        "            (The logarithm helps to dampen the effect of very high IDF values).\n",
        "\n",
        "        * **TF-IDF Score:** The product of TF and IDF.\n",
        "        \n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ImQJjYGLq2GE4eX40Mh28Q.png\" alt=\"TF-IDF Concept\" width=\"800\"/>\n",
        "\n",
        "    * **Pros:**\n",
        "        * Reduces the weight of very common words (like \"the\", \"a\") that occur in many documents and are less informative.\n",
        "        * Gives higher scores to words that are distinctive to a particular document.\n",
        "        * Generally performs better than simple BoW for tasks like information retrieval and text classification.\n",
        "    * **Cons:**\n",
        "        * Still disregards word order and semantic relationships (e.g., \"happy\" and \"joyful\" are treated as entirely different, unrelated terms).\n",
        "        * Can still result in high-dimensional sparse vectors.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3.  **Word Embeddings (e.g., Word2Vec, GloVe, FastText):**\n",
        "    * **Concept:** Represent words as dense, low-dimensional vectors (e.g., 50 to 300 dimensions, compared to potentially tens of thousands for BoW/TF-IDF). These vectors are learned in such a way that they capture semantic relationships between words. Words with similar meanings will have similar vector representations (i.e., they will be close to each other in the vector space).\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:1060/format:webp/1*LdviucnshWgIIcQvhTTF-g.png\" alt=\"Word Embeddings Vector Space\" width=\"600\"/>\n",
        "  \n",
        "    * **How they are learned (Intuition for Word2Vec):**\n",
        "        * These embeddings are typically learned using neural network models trained on vast amounts of text data.\n",
        "        * **CBOW (Continuous Bag-of-Words):** The model learns to predict a target word based on its surrounding context words.\n",
        "        * **Skip-gram:** The model learns to predict the surrounding context words given a target word.\n",
        "        * The \"weights\" learned by the hidden layer of these neural network architectures effectively become the word embedding vectors.\n",
        "    * **Key Property & Example:**\n",
        "        * They can capture analogies: `vector('king') - vector('man') + vector('woman') ‚âà vector('queen')`.\n",
        "        * Words like \"happy,\" \"joyful,\" and \"elated\" would have vectors that are close together in the embedding space.\n",
        "    * **Pros:**\n",
        "        * Capture semantic meaning and relationships between words (synonymy, analogy).\n",
        "        * Result in dense, lower-dimensional vectors, which can be more efficient for downstream ML models.\n",
        "        * Often lead to superior performance in complex NLP tasks such as sentiment analysis, machine translation, and question answering.\n",
        "        * Pre-trained word embeddings (trained on massive corpora like Wikipedia or Google News) are widely available, so you don't always need to train them from scratch.\n",
        "    * **Cons:**\n",
        "        * Training effective word embeddings from scratch requires very large text datasets and significant computational resources.\n",
        "        * Traditional word embeddings assign a single vector to each word, which can be problematic for words with multiple meanings (polysemy). (More advanced techniques like contextual embeddings ‚Äì e.g., ELMo, BERT ‚Äì address this).\n",
        "\n",
        "**Why is Text Vectorization Crucial?**\n",
        "Once text is converted into these numerical vectors, it can be fed into virtually any standard Machine Learning algorithm (e.g., Logistic Regression, SVMs for text classification; K-Means for document clustering; or advanced Neural Networks for sophisticated NLP tasks). This transformation bridges the gap between human language and machine computation.\n"
      ],
      "metadata": {
        "id": "l5Qc0MKp51nz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RPCyVuC-GyT8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}